{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /opt/miniconda3/lib/python3.13/site-packages (2.3.3)\n",
            "Requirement already satisfied: numpy in /opt/miniconda3/lib/python3.13/site-packages (2.3.4)\n",
            "Requirement already satisfied: scikit-learn in /opt/miniconda3/lib/python3.13/site-packages (1.7.2)\n",
            "Collecting geopandas\n",
            "  Using cached geopandas-1.1.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting shapely\n",
            "  Downloading shapely-2.1.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
            "Collecting fiona\n",
            "  Downloading fiona-1.10.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (56 kB)\n",
            "Collecting pyproj\n",
            "  Downloading pyproj-3.7.2-cp313-cp313-macosx_14_0_arm64.whl.metadata (31 kB)\n",
            "Collecting rtree\n",
            "  Using cached rtree-1.4.1-py3-none-macosx_11_0_arm64.whl.metadata (2.1 kB)\n",
            "Collecting xgboost\n",
            "  Downloading xgboost-3.1.2-py3-none-macosx_12_0_arm64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/miniconda3/lib/python3.13/site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/lib/python3.13/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/lib/python3.13/site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.8.0 in /opt/miniconda3/lib/python3.13/site-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/miniconda3/lib/python3.13/site-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/miniconda3/lib/python3.13/site-packages (from scikit-learn) (3.6.0)\n",
            "Collecting pyogrio>=0.7.2 (from geopandas)\n",
            "  Downloading pyogrio-0.11.1-cp313-cp313-macosx_12_0_arm64.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: packaging in /opt/miniconda3/lib/python3.13/site-packages (from geopandas) (25.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /opt/miniconda3/lib/python3.13/site-packages (from fiona) (25.4.0)\n",
            "Requirement already satisfied: certifi in /opt/miniconda3/lib/python3.13/site-packages (from fiona) (2025.10.5)\n",
            "Requirement already satisfied: click~=8.0 in /opt/miniconda3/lib/python3.13/site-packages (from fiona) (8.2.1)\n",
            "Collecting click-plugins>=1.0 (from fiona)\n",
            "  Using cached click_plugins-1.1.1.2-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting cligj>=0.5 (from fiona)\n",
            "  Using cached cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: six>=1.5 in /opt/miniconda3/lib/python3.13/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Using cached geopandas-1.1.1-py3-none-any.whl (338 kB)\n",
            "Downloading shapely-2.1.2-cp313-cp313-macosx_11_0_arm64.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fiona-1.10.1-cp313-cp313-macosx_11_0_arm64.whl (14.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.7/14.7 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pyproj-3.7.2-cp313-cp313-macosx_14_0_arm64.whl (4.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached rtree-1.4.1-py3-none-macosx_11_0_arm64.whl (436 kB)\n",
            "Downloading xgboost-3.1.2-py3-none-macosx_12_0_arm64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached click_plugins-1.1.1.2-py2.py3-none-any.whl (11 kB)\n",
            "Using cached cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading pyogrio-0.11.1-cp313-cp313-macosx_12_0_arm64.whl (19.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: shapely, rtree, pyproj, pyogrio, cligj, click-plugins, xgboost, fiona, geopandas\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9/9\u001b[0m [geopandas]/9\u001b[0m [geopandas]ins]\n",
            "\u001b[1A\u001b[2KSuccessfully installed click-plugins-1.1.1.2 cligj-0.7.2 fiona-1.10.1 geopandas-1.1.1 pyogrio-0.11.1 pyproj-3.7.2 rtree-1.4.1 shapely-2.1.2 xgboost-3.1.2\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install pandas numpy scikit-learn geopandas shapely fiona pyproj rtree xgboost\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting dbfread\n",
            "  Using cached dbfread-2.0.7-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Using cached dbfread-2.0.7-py2.py3-none-any.whl (20 kB)\n",
            "Installing collected packages: dbfread\n",
            "Successfully installed dbfread-2.0.7\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install dbfread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Loaded dataset: 227428 samples\n",
            "ğŸ“ Attaching REAL NYC borough boundaries...\n",
            "ğŸ™ï¸ Matched: 181871 check-ins to boroughs\n",
            "ğŸ“Š Building soft-label distributions...\n",
            "â¡ï¸ Created 837 unique contexts with probability distributions.\n",
            "ğŸ§  Training multi-output XGBoost Regressor...\n",
            "âœ¨ Validation MSE: 0.000337\n",
            "Bar                                         0.074189\n",
            "Hotel                                       0.048733\n",
            "Deli / Bodega                               0.032866\n",
            "Residential Building (Apartment / Condo)    0.031649\n",
            "Other Great Outdoors                        0.030818\n",
            "Subway                                      0.030680\n",
            "Medical Center                              0.030602\n",
            "Museum                                      0.029859\n",
            "German Restaurant                           0.029016\n",
            "Burger Joint                                0.025291\n",
            "Fast Food Restaurant                        0.024238\n",
            "Home (private)                              0.023779\n",
            "Latin American Restaurant                   0.023653\n",
            "Movie Theater                               0.021902\n",
            "Office                                      0.021422\n",
            "dtype: float32\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "from datetime import timedelta\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "import geopandas as gpd\n",
        "from shapely.geometry import Point\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ============================================================\n",
        "# 1. LOAD DATA\n",
        "# ============================================================\n",
        "\n",
        "def load_raw_data():\n",
        "    \"\"\"\n",
        "    Load the NYC Foursquare check-in dataset.\n",
        "    \"\"\"\n",
        "    file_path = \"/Users/chardog234/Documents/GitHub/Final_project_4003/dataset_TSMC2014_NYC.csv\"\n",
        "    df = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
        "    print(f\"âœ… Loaded dataset: {len(df)} samples\")\n",
        "    return df\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 2. NORMALIZE TIMES\n",
        "# ============================================================\n",
        "\n",
        "def normalize_times(df):\n",
        "    \"\"\"\n",
        "    Convert UTC timestamps to local time and extract:\n",
        "      - Day_of_Week\n",
        "      - Hour (0â€“23)\n",
        "    \"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    df[\"UTC_Time\"] = pd.to_datetime(\n",
        "        df[\"utcTimestamp\"],\n",
        "        utc=True,\n",
        "        format=\"%a %b %d %H:%M:%S +0000 %Y\",\n",
        "        errors=\"coerce\"\n",
        "    )\n",
        "\n",
        "    # Drop invalid timestamps\n",
        "    df = df.dropna(subset=[\"UTC_Time\"])\n",
        "\n",
        "    # Convert to local time using timezoneOffset (in minutes)\n",
        "    df[\"Local_Time\"] = df[\"UTC_Time\"] - pd.to_timedelta(df[\"timezoneOffset\"], unit=\"m\")\n",
        "    df[\"Day_of_Week\"] = df[\"Local_Time\"].dt.day_name()\n",
        "    df[\"Hour\"] = df[\"Local_Time\"].dt.hour\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 3. ATTACH BOROUGH SHAPES\n",
        "# ============================================================\n",
        "\n",
        "def attach_borough_zones(df):\n",
        "    \"\"\"\n",
        "    Spatially join check-ins to real NYC borough polygons.\n",
        "    Adds a 'Borough_ID' column containing the borough name.\n",
        "    \"\"\"\n",
        "    print(\"ğŸ“ Attaching REAL NYC borough boundaries...\")\n",
        "\n",
        "    shp = \"/Users/chardog234/Documents/GitHub/Final_project_4003/geo_export_3c9604c8-be29-4e21-ae16-9c7d64ebce63.shp\"\n",
        "    boroughs = gpd.read_file(shp).to_crs(epsg=4326)\n",
        "\n",
        "    BOROUGH_COL = \"boroname\"  # confirmed from DBF\n",
        "\n",
        "    # Convert (longitude, latitude) to geometry points\n",
        "    gdf = gpd.GeoDataFrame(\n",
        "        df.copy(),\n",
        "        geometry=[Point(xy) for xy in zip(df[\"longitude\"], df[\"latitude\"])],\n",
        "        crs=\"EPSG:4326\"\n",
        "    )\n",
        "\n",
        "    # Spatial join: each check-in gets assigned a borough\n",
        "    gdf_join = gpd.sjoin(\n",
        "        gdf,\n",
        "        boroughs[[BOROUGH_COL, \"geometry\"]],\n",
        "        how=\"left\",\n",
        "        predicate=\"within\"\n",
        "    )\n",
        "\n",
        "    gdf_join = gdf_join.rename(columns={BOROUGH_COL: \"Borough_ID\"})\n",
        "    gdf_join = gdf_join.dropna(subset=[\"Borough_ID\"])\n",
        "    gdf_join = gdf_join.drop(columns=[\"geometry\", \"index_right\"], errors=\"ignore\")\n",
        "\n",
        "    print(f\"ğŸ™ï¸ Matched: {len(gdf_join)} check-ins to boroughs\")\n",
        "    return gdf_join\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 4. BUILD SOFT-LABEL CONTEXTS\n",
        "# ============================================================\n",
        "\n",
        "def build_soft_contexts(df):\n",
        "    \"\"\"\n",
        "    Build soft-label probability distributions for each context:\n",
        "      Context = (Borough_ID, Day_of_Week, Hour)\n",
        "\n",
        "    For each context, compute:\n",
        "      P(venueCategory | borough, day, hour) = freq / total freq in that context\n",
        "\n",
        "    Returns a pivoted DataFrame with:\n",
        "      - columns: [Borough_ID, Day_of_Week, Hour, <venueCategory1>, <venueCategory2>, ...]\n",
        "      - each venueCategory column = probability in that context\n",
        "    \"\"\"\n",
        "    print(\"ğŸ“Š Building soft-label distributions...\")\n",
        "\n",
        "    group_cols = [\"Borough_ID\", \"Day_of_Week\", \"Hour\"]\n",
        "\n",
        "    # Count check-ins per (borough, day, hour, venueCategory)\n",
        "    counts = (\n",
        "        df.groupby(group_cols + [\"venueCategory\"])\n",
        "          .size()\n",
        "          .reset_index(name=\"freq\")\n",
        "    )\n",
        "\n",
        "    # Normalize frequencies within each context â†’ probabilities\n",
        "    counts[\"prob\"] = counts[\"freq\"] / counts.groupby(group_cols)[\"freq\"].transform(\"sum\")\n",
        "\n",
        "    # Pivot to wide format: 1 row per context, 1 column per venue category\n",
        "    df_pivot = counts.pivot_table(\n",
        "        index=group_cols,\n",
        "        columns=\"venueCategory\",\n",
        "        values=\"prob\",\n",
        "        fill_value=0.0\n",
        "    ).reset_index()\n",
        "\n",
        "    print(f\"â¡ï¸ Created {len(df_pivot)} unique contexts with probability distributions.\")\n",
        "    return df_pivot\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 5. MODEL (MULTI-OUTPUT REGRESSION)\n",
        "# ============================================================\n",
        "\n",
        "class MovementModel:\n",
        "    \"\"\"\n",
        "    Multi-output regression model that predicts\n",
        "    P(venueCategory | borough, day, hour)\n",
        "    as a probability distribution over categories.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.enc_borough = LabelEncoder()\n",
        "        self.enc_day = LabelEncoder()\n",
        "        self.enc_hour = LabelEncoder()\n",
        "        self.categories = None\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    def fit(self, df_ctx):\n",
        "        \"\"\"\n",
        "        Fit the model on context-level probability distributions.\n",
        "        \"\"\"\n",
        "        df = df_ctx.copy()\n",
        "\n",
        "        # Identify all venue category columns (everything except the keys)\n",
        "        self.categories = [c for c in df.columns if c not in [\"Borough_ID\", \"Day_of_Week\", \"Hour\"]]\n",
        "\n",
        "        # Encode inputs\n",
        "        df[\"borough_enc\"] = self.enc_borough.fit_transform(df[\"Borough_ID\"])\n",
        "        df[\"day_enc\"] = self.enc_day.fit_transform(df[\"Day_of_Week\"])\n",
        "        df[\"hour_enc\"] = self.enc_hour.fit_transform(df[\"Hour\"])\n",
        "\n",
        "        X = df[[\"borough_enc\", \"day_enc\", \"hour_enc\"]]\n",
        "        Y = df[self.categories]\n",
        "\n",
        "        X_train, X_test, Y_train, Y_test = train_test_split(\n",
        "            X, Y, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        print(\"ğŸ§  Training multi-output XGBoost Regressor...\")\n",
        "        base = XGBRegressor(\n",
        "            objective=\"reg:squarederror\",\n",
        "            n_estimators=300,\n",
        "            learning_rate=0.07,\n",
        "            max_depth=8,\n",
        "            subsample=0.9,\n",
        "            colsample_bytree=0.9\n",
        "        )\n",
        "\n",
        "        self.model = MultiOutputRegressor(base)\n",
        "        self.model.fit(X_train, Y_train)\n",
        "\n",
        "        preds = self.model.predict(X_test)\n",
        "        mse = mean_squared_error(Y_test.values.flatten(), preds.flatten())\n",
        "        print(f\"âœ¨ Validation MSE: {mse:.6f}\")\n",
        "\n",
        "    # --------------------------------------------------------\n",
        "    def predict_distribution(self, borough, day, hour, temperature=1.5):\n",
        "        \"\"\"\n",
        "        Predict a smoothed probability distribution over venue categories\n",
        "        for a given (borough, day, hour).\n",
        "\n",
        "        temperature > 1.0 â†’ softer probabilities (less peaky)\n",
        "        \"\"\"\n",
        "        X = pd.DataFrame([{\n",
        "            \"borough_enc\": self.enc_borough.transform([borough])[0],\n",
        "            \"day_enc\": self.enc_day.transform([day])[0],\n",
        "            \"hour_enc\": self.enc_hour.transform([hour])[0]\n",
        "        }])\n",
        "\n",
        "        raw = self.model.predict(X)[0]\n",
        "\n",
        "        # Clip small negatives due to regression noise\n",
        "        raw = np.maximum(raw, 0)\n",
        "\n",
        "        # Temperature smoothing to avoid extremely sharp peaks\n",
        "        raw = raw ** (1.0 / temperature)\n",
        "\n",
        "        # Re-normalize to sum to 1\n",
        "        total = raw.sum()\n",
        "        if total == 0:\n",
        "            # Fallback: uniform distribution if model is degenerate\n",
        "            probs = np.ones_like(raw) / len(raw)\n",
        "        else:\n",
        "            probs = raw / total\n",
        "\n",
        "        return pd.Series(probs, index=self.categories).sort_values(ascending=False)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# 6. MAIN SCRIPT (TEST RUN)\n",
        "# ============================================================\n",
        "\n",
        "def main():\n",
        "    # 1. Load and preprocess data\n",
        "    df = load_raw_data()\n",
        "    df = normalize_times(df)\n",
        "    df = attach_borough_zones(df)\n",
        "\n",
        "    # 2. Build soft probability contexts\n",
        "    df_ctx = build_soft_contexts(df)\n",
        "\n",
        "    # 3. Train model\n",
        "    model = MovementModel()\n",
        "    model.fit(df_ctx)\n",
        "\n",
        "    # 4. Example prediction\n",
        "    preds = model.predict_distribution(\"Manhattan\", \"Monday\", 10, temperature=1.5)\n",
        "    print(preds.head(15))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
